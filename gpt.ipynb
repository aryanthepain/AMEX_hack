{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeda62c8",
   "metadata": {},
   "source": [
    "# Click Prediction Preprocessing Pipeline\n",
    "\n",
    "This notebook outlines a preprocessing workflow for predicting the probability of a customer clicking an offer (target `y`) given they have seen it. After preprocessing, we perform PCA for dimensionality reduction and train a Naive Bayes classifier.\n",
    "\n",
    "**Steps covered:**\n",
    "1. Setup & Data Loading\n",
    "2. Class Distribution Analysis\n",
    "3. Dropping Unused ID Variables\n",
    "4. Missing-Value Handling\n",
    "5. Temporal Feature Engineering\n",
    "6. Encoding High-Cardinality Categories\n",
    "7. Feature Selection\n",
    "8. Train/Validation Split\n",
    "9. Saving Preprocessed Data to CSV\n",
    "10. PCA Dimensionality Reduction\n",
    "11. Naive Bayes Classification\n",
    "\n",
    "Each section contains markdown explanations and commented code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b0ddf",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "Import necessary libraries and load the first 500 rows for pipeline prototyping and the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f157fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # data manipulation\n",
    "import numpy as np   # numerical operations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load sample data (first 500 rows) and dictionary\n",
    "train_sample = pd.read_csv('output/train_df_head.csv')\n",
    "test_sample  = pd.read_csv('output/test_df_head.csv')\n",
    "# data_dict    = pd.read_csv('output/data_dict.csv')  # Uncomment if available\n",
    "\n",
    "print(f\"Train sample shape: {train_sample.shape}\")\n",
    "print(f\"Test sample shape:  {test_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b7aa7",
   "metadata": {},
   "source": [
    "## 2. Class Distribution Analysis\n",
    "\n",
    "Analyze the class distribution of the target variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class distribution of y\n",
    "class_counts = train_sample['y'].value_counts()\n",
    "class_props  = train_sample['y'].value_counts(normalize=True)\n",
    "print(\"Class Counts:\\n\", class_counts)\n",
    "print(\"\\nClass Proportions:\\n\", class_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873d9f8",
   "metadata": {},
   "source": [
    "## 3. Drop Unused ID Variables\n",
    "\n",
    "Drop ID columns that are not used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id1']\n",
    "train_sample.drop(columns=drop_cols, inplace=True)\n",
    "test_sample.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a360b",
   "metadata": {},
   "source": [
    "## 4. Missing-Value Handling\n",
    "\n",
    "Handle missing values by replacing sentinels, dropping high-missing features, and imputing with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Replace sentinel values with NaN\n",
    "num_cols = train_sample.select_dtypes(include=['int', 'float']).columns\n",
    "train_sample[num_cols] = train_sample[num_cols].replace(-9999, np.nan)\n",
    "test_sample[num_cols]  = test_sample[num_cols].replace(-9999, np.nan)\n",
    "\n",
    "# Drop features with >95% missing\n",
    "miss_pct = train_sample[num_cols].isna().mean()\n",
    "drop_high_miss = miss_pct[miss_pct > 0.95].index.tolist()\n",
    "train_sample.drop(columns=drop_high_miss, inplace=True)\n",
    "test_sample.drop(columns=drop_high_miss, inplace=True)\n",
    "\n",
    "# Impute with median and add indicators\n",
    "median_imp = SimpleImputer(strategy='median', add_indicator=True)\n",
    "train_sample[num_cols] = median_imp.fit_transform(train_sample[num_cols])\n",
    "test_sample[num_cols]  = median_imp.transform(test_sample[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15025a3",
   "metadata": {},
   "source": [
    "## 5. Temporal Feature Engineering\n",
    "\n",
    "Extract features from impression timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert impression timestamp to datetime and extract features\n",
    "train_sample['ts_imp'] = pd.to_datetime(train_sample['id4'])\n",
    "train_sample['hour']   = train_sample['ts_imp'].dt.hour\n",
    "train_sample['weekday']= train_sample['ts_imp'].dt.dayofweek\n",
    "train_sample['is_weekend'] = train_sample['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "test_sample['ts_imp'] = pd.to_datetime(test_sample['id4'])\n",
    "test_sample['hour']   = test_sample['ts_imp'].dt.hour\n",
    "test_sample['weekday']= test_sample['ts_imp'].dt.dayofweek\n",
    "test_sample['is_weekend'] = test_sample['weekday'].isin([5,6]).astype(int)\n",
    "\n",
    "# Drop raw timestamp\n",
    "train_sample.drop(columns=['id4','ts_imp'], inplace=True)\n",
    "test_sample.drop(columns=['id4','ts_imp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02475c",
   "metadata": {},
   "source": [
    "## 6. Encoding High-Cardinality Categories\n",
    "\n",
    "Encode high-cardinality categorical features using target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "tgt_enc = TargetEncoder(cols=['id2','id3'], smoothing=0.2)\n",
    "train_sample[['id2_enc','id3_enc']] = tgt_enc.fit_transform(train_sample[['id2','id3']], train_sample['y'])\n",
    "test_sample[['id2_enc','id3_enc']]  = tgt_enc.transform(test_sample[['id2','id3']])\n",
    "\n",
    "# Drop original IDs\n",
    "train_sample.drop(columns=['id2','id3'], inplace=True)\n",
    "test_sample.drop(columns=['id2','id3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2049a91",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "\n",
    "Select features using L1-regularized logistic regression (Lasso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_full = train_sample.drop(columns=['y'])\n",
    "y_full = train_sample['y']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "lasso = LogisticRegressionCV(penalty='l1', solver='saga', cv=3, scoring='roc_auc', max_iter=1000)\n",
    "lasso.fit(X_scaled, y_full)\n",
    "\n",
    "coef_mask = np.abs(lasso.coef_).ravel() > 1e-6\n",
    "selected_features = X_full.columns[coef_mask].tolist()\n",
    "print(f\"Selected {len(selected_features)} features out of {X_full.shape[1]}\")\n",
    "\n",
    "train_sel = train_sample[selected_features + ['y']]\n",
    "test_sel  = test_sample[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffa4aa",
   "metadata": {},
   "source": [
    "## 8. Train/Validation Split\n",
    "\n",
    "Split the data into training and validation sets using a time-based split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split using id5 timestamp\n",
    "train_sel['imp_time'] = pd.to_datetime(train_sample['id5'])\n",
    "train_sel.sort_values('imp_time', inplace=True)\n",
    "cutoff = int(len(train_sel)*0.8)\n",
    "\n",
    "train_final = train_sel.iloc[:cutoff].drop(columns=['imp_time'])\n",
    "valid_final = train_sel.iloc[cutoff:].drop(columns=['imp_time'])\n",
    "\n",
    "X_train, y_train = train_final.drop(columns=['y']), train_final['y']\n",
    "X_val,   y_val   = valid_final.drop(columns=['y']), valid_final['y']\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd367b6",
   "metadata": {},
   "source": [
    "## 9. Saving Preprocessed Data to CSV\n",
    "\n",
    "Save the preprocessed train, validation, and test sets to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.to_csv('output/preprocessed_train.csv', index=False)\n",
    "valid_final.to_csv('output/preprocessed_valid.csv', index=False)\n",
    "test_sel.to_csv('output/preprocessed_test.csv', index=False)\n",
    "print(\"Saved preprocessed CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2bd63",
   "metadata": {},
   "source": [
    "## 10. PCA Dimensionality Reduction\n",
    "\n",
    "Apply PCA on scaled training features, retaining 95% variance, then transform validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features first\n",
    "all_features = selected_features\n",
    "scaler_pca = StandardScaler()\n",
    "X_train_scaled = scaler_pca.fit_transform(X_train[all_features])\n",
    "X_val_scaled   = scaler_pca.transform(X_val[all_features])\n",
    "X_test_scaled  = scaler_pca.transform(test_sel[all_features])\n",
    "\n",
    "# Fit PCA to 95% variance\n",
    "pca = PCA(n_components=0.95, svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca   = pca.transform(X_val_scaled)\n",
    "X_test_pca  = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"PCA components: {pca.n_components_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbadce",
   "metadata": {},
   "source": [
    "## 11. Naive Bayes Classification\n",
    "\n",
    "Train a Gaussian Naive Bayes on the PCA-transformed data and evaluate on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "# Fit\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict probabilities and evaluate MAP (average precision)\n",
    "y_val_pred = nb.predict_proba(X_val_pca)[:,1]\n",
    "map7 = average_precision_score(y_val, y_val_pred)\n",
    "print(f\"Validation Average Precision (Proxy for MAP@7): {map7:.4f}\")\n",
    "\n",
    "# Save predictions for test set\n",
    "y_test_pred = nb.predict_proba(X_test_pca)[:,1]\n",
    "pd.DataFrame({'y_pred': y_test_pred}).to_csv('output/nb_test_predictions.csv', index=False)\n",
    "print(\"Saved Naive Bayes test predictions.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
